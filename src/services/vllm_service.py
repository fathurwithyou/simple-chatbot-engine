
import httpx
from core.config import VLLM_API_URL, VLLM_DEFAULT_MODEL
from api.v1.models import LLMRequest
from core.exceptions import LLMServiceError, ModelNotFoundError
from fastapi import status

async def generate_with_vllm(request: LLMRequest) -> str:
    model_name = request.model if request.model else VLLM_DEFAULT_MODEL
    url = VLLM_API_URL
    headers = {"Content-Type": "application/json"}
    payload = {
        "prompt": request.prompt,
        "n": 1,
        "max_tokens": request.max_tokens,
        "temperature": request.temperature,
    }

    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(url, json=payload, timeout=600.0)
            response.raise_for_status()
            result = response.json()

            if result and result.get("text"):
                return result["text"][0].strip()

            raise LLMServiceError(service_name="vLLM",
                                  original_detail="No text generated by vLLM.")
        except httpx.RequestError as exc:
            raise LLMServiceError(service_name="vLLM",
                                  original_detail=str(exc))
        except httpx.HTTPStatusError as exc:
            if exc.response.status_code == status.HTTP_404_NOT_FOUND:
                raise ModelNotFoundError(
                    model_name=model_name, engine_name="vLLM")
            raise LLMServiceError(service_name="vLLM",
                                  original_detail=exc.response.text)
        except Exception as exc:
            raise LLMServiceError(service_name="vLLM",
                                  original_detail=f"Unexpected error: {exc}")
